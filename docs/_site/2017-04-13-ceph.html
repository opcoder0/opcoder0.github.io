<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <link rel="stylesheet" href="/assets/css/style.css?v=" media="screen" type="text/css">
    <link rel="stylesheet" href="/assets/css/print.css" media="print" type="text/css">

    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Opcoder0 Blog Posts | Technical Articles and Blog Posts</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="Opcoder0 Blog Posts" />
<meta name="author" content="opcoder0" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Technical Articles and Blog Posts" />
<meta property="og:description" content="Technical Articles and Blog Posts" />
<link rel="canonical" href="http://localhost:4000/2017-04-13-ceph.html" />
<meta property="og:url" content="http://localhost:4000/2017-04-13-ceph.html" />
<meta property="og:site_name" content="Opcoder0 Blog Posts" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Opcoder0 Blog Posts" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"opcoder0"},"description":"Technical Articles and Blog Posts","headline":"Opcoder0 Blog Posts","url":"http://localhost:4000/2017-04-13-ceph.html"}</script>
<!-- End Jekyll SEO tag -->


    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" -->

<!-- end custom head snippets -->

  </head>

  <body>
    <header>
      <div class="inner">
        <a href="http://localhost:4000/">
          <h1>Opcoder0 Blog Posts</h1>
        </a>
        <h2>Technical Articles and Blog Posts</h2>
        
        
      </div>
    </header>

    <div id="content-wrapper">
      <div class="inner clearfix">
        <section id="main-content">
          <h2 id="ceph-sandbox---install-ceph-on-lxc">CEPH Sandbox - Install CEPH on LXC</h2>

<p>CEPH is a free and opensource object storage system. This article describes the basic terminology, installation and configuration parameters required to build your own CEPH environment. CEPH has been designed to be a distributed storage system which is highly fault tolerant, scalable and configurable. It can run on large number of distributed commodity hardware thus eliminating the need for very large central storage solutions. This post aims to be a basic, short and self contained article that explains all the key details to understand and play with CEPH.</p>

<h3 id="environment">Environment</h3>

<p>I’ve setup CEPH on my laptop with a few LXC containers. My setup has –</p>

<p>Host: Ubuntu 16.04 xenial 64-bit OS
LXC version: 2.0.6
40G disk partition that is free to use for this experiement.
CEPH Release – Jewel</p>

<p>The Ubuntu site has documentation on LXC configuration here. But this site talks about creating unprivileged containers. However, we’ll need “privileged” containers. Privileged containers are’t considered secure, since processes are mapped to root user on the host. Hence it has been used only for the purpose of experimenting.</p>

<p>See documentation on LXC site here to understand how to create privileged LXC containers.</p>

<p>Now, create these containers (as privileged) with names like these –</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cephadmin
cephmon
cephosd0
cephosd1
cephosd2
cephradosgw
</code></pre></div></div>

<p>Here’s a command to create a container with Ubuntu Xenial 64-bit container. You could choose another distro, in which case some of the instructions might not be applicable. Using the names above run the command to create each container –</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>host:~# lxc-create -n &lt;container-name&gt; -t download -- -d ubuntu -r xenial -a amd64
</code></pre></div></div>

<p>Though CEPH installations talk about private and public networks. For testing purposes, the <code class="language-plaintext highlighter-rouge">lxcbr0</code> available from LXC is sufficient to successfully work with CEPH.</p>

<p>Start the containers and install SSH servers on all of them –</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>host:~# lxc-start -n &lt;container-name&gt;
host:~# lxc-attach -n &lt;container-name&gt;
container:~# apt-get install openssh-server
container:~# exit
</code></pre></div></div>

<p>Repeat the above commands for all the containers you’ve created. Re-start all the containers. Register the container-names into your /etc/hosts for easily being able to login to them.</p>

<h3 id="disk-setup">Disk Setup</h3>

<p>On the 40G disk partition you’ve allocated for this exercise perform –</p>

<ul>
  <li>Delete the existing partition</li>
  <li>Create 3 new partitions</li>
  <li>Format each of them with XFS filesystem.</li>
</ul>

<p>CEPH recommends using XFS / BTRFS / EXT4. I’ve used XFS in my tests. I tried with EXT4 but received warnings related to limited xattr sizes while CEPH was being deployed.</p>

<p>Note down the device major and minor numbers for the partitions you’ve created from the above steps.</p>

<p>Make each of the partition you’ve created available to each of the <code class="language-plaintext highlighter-rouge">cephosd0, cephosd1</code> and <code class="language-plaintext highlighter-rouge">cephosd2</code> containers. If the above operation resulted in <code class="language-plaintext highlighter-rouge">/dev/sda8</code>, <code class="language-plaintext highlighter-rouge">/dev/sda9</code> and <code class="language-plaintext highlighter-rouge">/dev/sda10</code> devices. Assign each of the device major and minor number to a corresponding <code class="language-plaintext highlighter-rouge">cephosd&lt;N&gt;</code> container. To do that, you’ll need to edit the LXC configuration file for each of the cephosd container by performing the steps –</p>

<p>Login to each cephosd<N> node, and run</N></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> container:~# mkdir -p /mnt/xfsdisk
</code></pre></div></div>
<p>Stop your cephosd containers (cephosd0, cephosd1, cephosd2) –</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> host:~# lxc-stop -n cephosd&lt;N&gt;
</code></pre></div></div>

<p>On the host create corresponding ‘fstab’ file for each container. You’d assign /dev/sda8 to cephosd0, /dev/sda9 to cephosd1 and so on. The syntax of fstab would have lines like this depending on how many disks / partitions you intend to share –</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/dev/sda&lt;N&gt; &lt;mount-point-in-container&gt; &lt;file-system&gt; &lt;options&gt;
</code></pre></div></div>

<p>Example –</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/dev/sda8 mnt/xfsdisk xfs noatime 0 0
</code></pre></div></div>

<p><strong>IMPORTANT NOTE</strong></p>

<p>The <mount-point-in-container> has no preceding forward slash. The preceding slash is not required. Refer this post on askubuntu.com for more details. For each of the cephosd<N> node edit the configuration file</N></mount-point-in-container></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>host:~# cd /var/lib/lxc
host:~# cd cephosd&lt;N&gt;
</code></pre></div></div>

<p>Edit the container configuration file <code class="language-plaintext highlighter-rouge">config</code>. The lines below gives the container permission to access the device inside LXC and mount it to the mount point defined in the <code class="language-plaintext highlighter-rouge">fstab</code> file. Add the following lines –</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>lxc.cgroup.devices.allow = b &lt;maj&gt;:&lt;min&gt; rwm
lxc.aa_profile = unconfined
lxc.mount = /var/lib/lxc/cephosd&lt;N&gt;/fstab
</code></pre></div></div>

<p>Now, start/restart the containers. With the above set of steps we complete the creation of containers ready for us to install CEPH.</p>

<h3 id="ceph-installation">CEPH Installation</h3>

<p>Complete the pre-flight steps on the CEPH quick install from here. The steps in pre-flight –</p>

<ul>
  <li>Setup the ‘cephadmin’ node with the ceph-deploy package.</li>
  <li>Installs ‘ntp’ on all the nodes (required where OSD or MON run).</li>
  <li>Create a common ceph deploy user (password-less ssh sudo access) that will be used for CEPH installation on all nodes.</li>
</ul>

<p>After the pre-flight steps are complete check –</p>

<ul>
  <li>To ensure the password-less access works from ‘cephadmin’ to all the other nodes on your cluster via the ceph deploy user you have created.</li>
  <li>The XFS partition created earlier is available on all OSDs under /mnt/xfsdisk.</li>
</ul>

<p>Next, you need to complete the CEPH deployment. The steps with all the illustrations are available here. To get a good understanding of each command read the description from the link. Since our experiement has 3 OSDs and 3 monitor daemons; You could run these commands from the ‘cephadmin’ node using the ceph deploy user created in pre-flight –</p>

<ul>
  <li>Create a directory and run the commands below from within it.</li>
  <li>Designate following nodes as CEPH Monitors</li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cephadmin:~$ /usr/bin/ceph-deploy new cephmon cephosd0 cephosd1
</code></pre></div></div>

<p>Install CEPH on all nodes including admin node.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cephadmin:~$ /usr/bin/ceph-deploy install cephadmin cephmon cephosd0 cephosd1 cephosd2 cephradosgw
</code></pre></div></div>

<p>Add the initial monitors and gather their keys</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cephadmin~:$ /usr/bin/ceph-deploy mon create-initial
</code></pre></div></div>

<p>Prepare the disk on each OSD</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cephadmin~:$ /usr/bin/ceph-deploy osd prepare cephosd0:/mnt/xfsdisk cephosd1:/mnt/xfsdisk cephosd2:/mnt/xfsdisk
</code></pre></div></div>

<p>Activate each OSD</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cephadmin~:$ /usr/bin/ceph-deploy osd activate cephosd0:/mnt/xfsdisk cephosd1:/mnt/xfsdisk cephosd2:/mnt/xfsdisk
</code></pre></div></div>

<p>Run this command to ensure cephadmin can perform administrative activities on all your nodes on the CEPH cluster</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cephadmin~:$ /usr/bin/ceph-deploy admin cephadmin cephmon cephosd0 cephosd1 cephosd2 cephradosgw
</code></pre></div></div>

<p>Check health of cluster (You must see HEALTH_OK) if you’ve performed all the steps correctly.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cephadmin~:$ ceph health
</code></pre></div></div>
<p>Install and deploy the instance of RADOS gateway</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cephadmin~:$ /usr/bin/ceph-deploy install --rgw cephradosgw
cephadmin~:$ /usr/bin/ceph-deploy rgw create cephradosgw
</code></pre></div></div>

<p><strong>NOTE</strong></p>

<p>If you haven’t configured the OSD daemons to start automatically via upstart, they won’t after a LXC startup/restart. And running <code class="language-plaintext highlighter-rouge">ceph -s</code> or <code class="language-plaintext highlighter-rouge">ceph -w</code> or <code class="language-plaintext highlighter-rouge">ceph health</code> on admin node shows HEALTH_ERR and degraded cluster since the OSDs are down. In such a case, manually login to each OSD and start the OSD instance with the command <code class="language-plaintext highlighter-rouge">sudo systemctl start ceph-osd@&lt;instance&gt;</code> <instance> in our case is 0, 1, or 2.</instance></p>

<p>With the above steps, the cluster must be up with all the PGs showing ‘active + clean’ status;</p>

<h3 id="access-data-on-cluster">Access data on cluster</h3>

<p>To run these commands you’ll need a subset ceph.conf with monitor node information and admin keyring file. For test purposes you could run it on the cephadmin node under the cluster directory you’ve created in the first step of CEPH installation section.</p>

<p>To list all available data pools</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cephadmin~:$ rados lspools
</code></pre></div></div>

<p>To create a pool</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cephadmin~:$ rados mkpool &lt;pool-name&gt;
</code></pre></div></div>

<p>To insert data in a file into a pool</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cephadmin~:$ rados put {object-name} {file-path} --pool=&lt;pool-name&gt;
</code></pre></div></div>

<p>To list all objects in a pool</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cephadmin~:$ rados -p &lt;pool-name&gt; ls
</code></pre></div></div>

<p>To get a data object from the pool</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cephadmin~:$ rados get {object-name} {outfile-path} --pool=&lt;pool-name&gt;
</code></pre></div></div>

	  <hr/>
        </section>

        <aside id="sidebar">
          

          
	  <nav>
    <br/><br/><br/>
    
    <a href="/"  class="inactive" > Home </a><br/>
    
    <a href="/about.html"  class="inactive" > About </a><br/>
    
    <a href="/post.html"  class="inactive" > Blogs </a><br/>
    
</nav>

        </aside>
      </div>
    </div>

  </body>
</html>
